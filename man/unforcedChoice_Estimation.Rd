% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/estimationRoutines.R
\name{unforcedChoice_Estimation}
\alias{unforcedChoice_Estimation}
\title{Estimates the parameters for unforced choice SDT models.}
\usage{
unforcedChoice_Estimation(
  data,
  fixed = NULL,
  rates = TRUE,
  objective = "maxLik",
  nInit = 5,
  startingPars = NULL,
  control = list(trace = 3),
  ...
)
}
\arguments{
\item{data}{: data.frame. Must have columns:
'model': character. Can be any of 'IO', 'DO', 'Ens', 'Int', 'Custom'
'nTP': numeric. Number of target present trials for the row.
'nTA': numeric. Number of target absent trials for the row.
'nSize': numeric. Number of stimuli presented on each trial for the row.
'criterion': If numeric and fixed includes 'criterion', values that will be used for computations for the row.
             Otherwise, it will be treated as a label of a criterion to be estimated equal across rows with the same value.
'mu': If numeric and fixed includes 'mu', values that will be used for computations for the row.
             Otherwise, it will be treated as a label of a mu to be estimated equal across rows with the same value.
'sigmaT': If numeric and fixed includes 'sigmaT', values that will be used for computations for the row.
             Otherwise, it will be treated as a label of a sigmaT to be estimated equal across rows with the same value.
'rho_0': If numeric and fixed includes 'rho_0', values that will be used for computations for the row.
             Otherwise, it will be treated as a label of a rho_0 to be estimated equal across rows with the same value.
'rho_1': If numeric and fixed includes 'rho_1', values that will be used for computations for the row.
             Otherwise, it will be treated as a label of a rho_1 to be estimated equal across rows with the same value.
'HR' : Numeric. Either a value between 0 and 1 given the Hit Rate or an integer giving the number of Hits for the row.
'FAR' : Numeric. Either a value between 0 and 1 given the False Alarm Rate or an integer giving the number of Misses for the row.
'RRTP' : Numeric. Either a value between 0 and 1 given the Rejection Rate on target present trials or an integer giving the number of Incorrect Rejections for the row.
'RRTA' : Numeric. Either a value between 0 and 1 given the Rejection Rate on target absent or an integer giving the number of Correct Rejections for the row.}

\item{fixed}{: Optional character vector. Name of parameter that will fixed at values in data data.frame. Can include any of
'criterion', 'mu', 'sigmaT', 'rho_0', 'rho_1'.}

\item{rates}{: Logical. Indicates whether columns HR, FAR, RRTP, and RRTA of data ought to be treated as proportions/rates (default) or as counts.}

\item{objective}{: Character. Whether to estimate by maximum likelihood, 'maxLik' (default) or by weighed least squares 'wls'}

\item{nInit}{: Numeric. Number of different initial parameters to try for the optimization routine. Defaults to 5. Should be at least 1}

\item{control}{: List. control parameters to be passed to optim. Default is list(trace = 3)). See ?optim for details.}

\item{...}{: additional parameters to be passed to the model specific functions.}

\item{startingPars:}{Numeric vector. Starting values to use for (at least one of) the optimization runs.}
}
\value{
output
}
\description{
Estimates the parameters for unforced choice SDT models.
}
\examples{
# Not run:
          # (exampleIO.ml    <- unforcedChoice_Estimation(data = toyIO,
          #                                               fixed = c("rho_0", "rho_1"),
          #                                               objective = "maxLik"))
          # (exampleIO.c2    <- unforcedChoice_Estimation(data = toyIO,
          #                                               fixed = c("rho_0", "rho_1"),
          #                                               objective = "wls"))
          # (exampleDO.ml    <- unforcedChoice_Estimation(data = toyDO,
          #                                               fixed = c("r1=r0"),
          #                                               objective = "maxLik"))
          # (exampleDO.c2    <- unforcedChoice_Estimation(data = toyDO,
          #                                               fixed = c("r1=r0"),
          #                                               objective = "wls"))
          # (exampleInt.ml   <- unforcedChoice_Estimation(data = toyInt,
          #                                               fixed = c("r1=r0"),
          #                                               objective = "maxLik"))
          # (exampleInt.ml.r <- unforcedChoice_Estimation(data = toyInt,
          #                                               fixed = c("r1=r0", "rho_0"),
          #                                               objective = "maxLik"))
          # (exampleInt.ml.s <- unforcedChoice_Estimation(data = toyInt,
          #                                               fixed = c("r1=r0", "sigmaT"),
          #                                               objective = "maxLik"))
          # (exampleInt.c2   <- unforcedChoice_Estimation(data = toyInt,
          #                                               fixed = c("r1=r0"),
          #                                               objective = "wls"))
          # (exampleInt.c2.r <- unforcedChoice_Estimation(data = toyInt,
          #                                               fixed = c("r1=r0", "rho_0"),
          #                                               objective = "wls"))
          # (exampleInt.c2.s <- unforcedChoice_Estimation(data = toyInt,
          #                                               fixed = c("r1=r0", "sigmaT"),
          #                                               objective = "wls"))
          # (exampleEns.ml   <- unforcedChoice_Estimation(data = toyEns,
          #                                               fixed = c("r1=r0"),
          #                                               method.Ens = "mvnorm",
          #                                               objective = "maxLik"))
          # (exampleEns.ml.r <- unforcedChoice_Estimation(data = toyEns,
          #                                               fixed = c("r1=r0", "rho_0"),
          #                                               method.Ens = "mvnorm",
          #                                               objective = "maxLik"))
          # (exampleEns.ml.s <- unforcedChoice_Estimation(data = toyEns,
          #                                               fixed = c("r1=r0", "sigmaT"),
          #                                               method.Ens = "mvnorm",
          #                                               objective = "maxLik"))
          # (exampleEns.c2   <- unforcedChoice_Estimation(data = toyEns,
          #                                               fixed = c("r1=r0"),
          #                                               method.Ens = "mvnorm", objective = "wls"))
          # (exampleEns.c2.r <- unforcedChoice_Estimation(data = toyEns,
          #                                               fixed = c("r1=r0", "rho_0"),
          #                                               method.Ens = "mvnorm", objective = "wls"))
          # (exampleEns.c2.s <- unforcedChoice_Estimation(data = toyEns,
          #                                               fixed = c("r1=r0", "sigmaT"),
          #                                               method.Ens = "mvnorm", objective = "wls"))

}
